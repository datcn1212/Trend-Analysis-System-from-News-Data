{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8176aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'456'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"123456\"\n",
    "\n",
    "a[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38632a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-13 07:00:00\n",
      "1683936000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# timestamp = 1683936000\n",
    "timestamp = 1683936000\n",
    "date = datetime.datetime.fromtimestamp(timestamp)\n",
    "\n",
    "print(date)\n",
    "print(int(date.timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "633aa56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-13 07:00:00\n",
      "1683936000\n"
     ]
    }
   ],
   "source": [
    "# e this\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2f8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c66c105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "b=[\"a\"]\n",
    "if b:\n",
    "    print(b[-1])\n",
    "else: print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc4e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/14 23:36:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/14 23:36:11 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://localhost:9000/user/root/data1.parquet.\n",
      "java.net.ConnectException: Call From canada-ubuntu/192.168.1.8 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n",
      "\tat com.sun.proxy.$Proxy34.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy35.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n",
      "\t... 40 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o25.parquet.\n: java.net.ConnectException: Call From canada-ubuntu/192.168.1.8 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m hdfs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://localhost:9000/user/root/data1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read the Parquet data from HDFS\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdfs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform operations on the DataFrame (e.g., display schema, show data)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.parquet.\n: java.net.ConnectException: Call From canada-ubuntu/192.168.1.8 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy34.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy35.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Specify the HDFS path to the Parquet file\n",
    "hdfs_path = \"hdfs://localhost:9000/user/root/data1.parquet\"\n",
    "\n",
    "# Read the Parquet data from HDFS\n",
    "df = spark.read.parquet(hdfs_path)\n",
    "\n",
    "# Perform operations on the DataFrame (e.g., display schema, show data)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484685bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                .appName(\"Write to HDFS\") \\\n",
    "                .getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "hdfs_path = \"hdfs://localhost:9000/user/root/data1.parquet\"\n",
    "df.write.parquet(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4a38e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['scala', 'spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 's' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eecc7947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6b190d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f027c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5227841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"save to HDFS\").getOrCreate()\n",
    "schema = StructType([\n",
    "                        StructField(\"Topic\", StringType(), nullable=False),\n",
    "                        StructField(\"Date\", StringType(), nullable=True),\n",
    "                        StructField(\"Author\", StringType(), nullable=True),\n",
    "                        StructField(\"Title\", StringType(), nullable=False),\n",
    "                        StructField(\"Href\", StringType(), nullable=False),\n",
    "                        StructField(\"Description\", StringType(), nullable=True),\n",
    "                        StructField(\"Body\", StringType(), nullable=False)\n",
    "                    ])\n",
    "data =[{\"Topic\": \"Thời sự\", \"Date\": \"Thứ sáu, 3/2/2023, 17:31 (GMT+7)\", \"Author\": \"Viết Tuân\", \"Title\": \"Tiếp tục sáp nhập huyện, xã \", \"Href\": \"https://vnexpress.net/tiep-tuc-sap-nhap-huyen-xa-4566487.html\", \"Description\": \"Bộ Chính trị yêu cầu tiếp tục sắp xếp đơn vị hành chính cấp huyện, xã đến năm 2030, trong đó khuyến khích địa phương tự đề xuất sáp nhập phù hợp thực tiễn.\", \"Body\": \"Ngày 30/1, Thường trực Ban Bí thư Võ Văn Thưởng thay mặt Bộ Chính trị ký ban hành kết luận về tiếp tục sắp xếp đơn vị hành chính cấp huyện, xã giai đoạn 2023-2030. Theo đó, đến năm 2025, toàn quốc hoàn thành sáp nhập huyện, xã có dân số và diện tích dưới 70% quy định; huyện diện tích dưới 20%, dân số dưới 200% quy định; xã diện tích dưới 20% và dân số dưới 300% quy định. Năm 2030, toàn quốc hoàn thành sáp nhập huyện, xã còn lại có diện tích và dân số dưới chuẩn; huyện diện tích dưới 30% và dân số dưới 200% quy định; xã diện tích dưới 30% và dân số dưới 300% quy định. Tiêu chuẩn của huyện miền núi, vùng cao là có 80.000 người và diện tích 850 km2 trở lên; huyện đồng bằng từ 450 km2; quận từ 35 km2 với dân số ít nhất 150.000. Quy mô dân số của xã 5.000-8.000 người trở lên, diện tích từ 30 km2. Việc sắp xếp huyện, xã thời gian tới phải phù hợp quy hoạch tỉnh, nông thôn, đô thị; xác định rõ lộ trình, đảm bảo đồng thuận của nhân dân. Các địa phương được khuyến khích chủ động đề xuất sắp xếp đơn vị hành chính tinh gọn, phù hợp thực tiễn, kể cả những nơi đã đảm bảo tiêu chuẩn. Các đơn vị hành chính đã sắp xếp giai đoạn trước; ổn định từ lâu, có vị trí biệt lập, yếu tố đặc thù; đơn vị hành chính nông thôn đã được quy hoạch thành đô thị không bắt buộc sáp nhập, trừ khi địa phương có nhu cầu. Cùng với tiếp tục sáp nhập huyện, xã, Bộ Chính trị yêu cầu cấp ủy đảng, chính quyền tổng kết các vấn đề đã rõ, được thực tiễn chứng minh là đúng để hoàn thiện văn bản quy phạm pháp luật, tổ chức thực hiện hiệu quả trong giai đoạn tới. Bộ Chính trị yêu cầu quy định rõ việc sử dụng và lộ trình sắp xếp cán bộ, công chức, số cấp phó dôi dư sau sáp nhập; định mức phân bổ ngân sách cho huyện, xã sau sáp nhập; thời gian hưởng chế độ, chính sách hỗ trợ đặc thù; hỗ trợ đầu tư xây dựng cơ bản. Đảng đoàn Quốc hội được giao chỉ đạo sửa đổi, bổ sung các văn bản để triển khai chủ trương này. Ban cán sự đảng Chính phủ cần nghiên cứu chính sách phù hợp tạo thuận lợi để địa phương sắp xếp huyện, xã. Giai đoạn 2019-2021, toàn quốc đã sắp xếp 21 đơn vị hành chính cấp huyện và 1.056 đơn vị cấp xã, qua đó giảm được 8 huyện và 561 xã. Việc sắp xếp đơn vị hành chính đã giảm 3.437 cơ quan ở cấp xã và 429 cơ quan cấp huyện; tinh giản 3.595 biên chế cấp xã và 141 biên chế cấp huyện; giảm chi ngân sách giai đoạn 2019-2021 hơn 2.000 tỷ đồng.\"}\n",
    "      ]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "# Save DataFrame as JSON to HDFS\n",
    "output_path = \"hdfs://localhost:9000/user/root/test5.parquet\"\n",
    "df.write.json(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eb7f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Author: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Href: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Topic: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.json(\"hdfs://localhost:9000/user/root/test1.parquet\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afdbc786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Author='Đắc Thành', Body='Cù Lao Chàm có 8 hòn đảo, cách bờ biển Cửa Đại, TP Hội An 18 km, được UNESCO công nhận khu dự trữ sinh quyển. Nơi đây phân chia thành ba vùng chức năng, trong đó vùng lõi rộng 2.471 hecta thuộc khu bảo tồn biển Cù Lao Chàm, với đặc trưng là các hệ sinh thái rừng và biển như rạn san hô, thảm cỏ biển, rừng đặc dụng. Vùng đệm rộng 8.455 hecta và vùng chuyển tiếp 22.220 hecta. Năm 1996, các rạn san hô ở Cù lao Chàm phân bố chủ yếu ở phía tây, tây nam đảo Hòn Lao và xung quanh các đảo nhỏ với tổng diện tích khoảng 165 hecta mặt nước có 135 loài san hô với 35 giống, trong đó có 6 loài lần đầu được tìm thấy ở vùng biển Việt Nam. Thời điểm đó, các rạn san hô đều bị hư hại do đánh bắt hải sản, khai thác để trang trí, xây nhà, nung vôi làm vật liệu xây dựng. Cùng với tác động của thiên tai, san hô Cù lao Chàm suy giảm cả về diện tích và vẻ đẹp. Cù Lao Chàm có 8 hòn đảo, cách bờ biển Cửa Đại, TP Hội An 18 km, được UNESCO công nhận khu dự trữ sinh quyển. Nơi đây phân chia thành ba vùng chức năng, trong đó vùng lõi rộng 2.471 hecta thuộc khu bảo tồn biển Cù Lao Chàm, với đặc trưng là các hệ sinh thái rừng và biển như rạn san hô, thảm cỏ biển, rừng đặc dụng. Vùng đệm rộng 8.455 hecta và vùng chuyển tiếp 22.220 hecta. Năm 1996, các rạn san hô ở Cù lao Chàm phân bố chủ yếu ở phía tây, tây nam đảo Hòn Lao và xung quanh các đảo nhỏ với tổng diện tích khoảng 165 hecta mặt nước có 135 loài san hô với 35 giống, trong đó có 6 loài lần đầu được tìm thấy ở vùng biển Việt Nam. Thời điểm đó, các rạn san hô đều bị hư hại do đánh bắt hải sản, khai thác để trang trí, xây nhà, nung vôi làm vật liệu xây dựng. Cùng với tác động của thiên tai, san hô Cù lao Chàm suy giảm cả về diện tích và vẻ đẹp. Năm 2011, Ban Quản lý khu bảo tồn biển Cù Lao Chàm được Viện Hải dương học Nha Trang chuyển giao công nghệ, kỹ thuật phục hồi san hô. Dựa trên vốn sẵn có, Ban quản lý đã bảo vệ những vùng san hô đang phát triển rồi lấy giống để ươm trồng. Năm 2011, Ban Quản lý khu bảo tồn biển Cù Lao Chàm được Viện Hải dương học Nha Trang chuyển giao công nghệ, kỹ thuật phục hồi san hô. Dựa trên vốn sẵn có, Ban quản lý đã bảo vệ những vùng san hô đang phát triển rồi lấy giống để ươm trồng. Một nhánh san hô được lấy lên và kiểm tra trước khi đưa đi ươm giống. Một nhánh san hô được lấy lên và kiểm tra trước khi đưa đi ươm giống. Nhánh cây san hô sừng hươu cho vào ống nhựa và cố định bằng đinh vít để đưa xuống vườn ươm. Nhánh cây san hô sừng hươu cho vào ống nhựa và cố định bằng đinh vít để đưa xuống vườn ươm. Vườn ươm giống san hô cấy trong giá thể là những ống nhựa PVC và hàn thành những khung rộng khoảng 3 m2, trên đó gắn những đoạn ống cao 12 cm, cách nhau 40 cm để đón những nhành san hô cấy vào. Vườn ươm giống san hô cấy trong giá thể là những ống nhựa PVC và hàn thành những khung rộng khoảng 3 m2, trên đó gắn những đoạn ống cao 12 cm, cách nhau 40 cm để đón những nhành san hô cấy vào. Khu vực vườn ươm sâu 4-6 m, tỷ lệ sống 70-90%. Nhân viên Ban Quản lý khu bảo tồn biển Cù Lao Chàm thường xuyên lặn xuống kiểm tra. Công việc này đòi hỏi kỹ năng lặn và sức khỏe tốt. \"Từ mùa xuân đến mùa hè là thời điểm thích hợp cho việc lặn trồng và san hô phát triển để chống chọi trong mùa mưa bão\", chị Nguyễn Thị Hồng Thúy, nhân viên Ban Quản lý phụ trách phục hồi san hô, cho biết. Khu vực vườn ươm sâu 4-6 m, tỷ lệ sống 70-90%. Nhân viên Ban Quản lý khu bảo tồn biển Cù Lao Chàm thường xuyên lặn xuống kiểm tra. Công việc này đòi hỏi kỹ năng lặn và sức khỏe tốt. \"Từ mùa xuân đến mùa hè là thời điểm thích hợp cho việc lặn trồng và san hô phát triển để chống chọi trong mùa mưa bão\", chị Nguyễn Thị Hồng Thúy, nhân viên Ban Quản lý phụ trách phục hồi san hô, cho biết.', Date='Thứ bảy, 28/1/2023, 04:00 (GMT+7)', Description='Sau nhiều năm bảo vệ và trồng mới, diện tích san hô ở Cù Lao Chàm đã tăng gấp hai lần so với năm 1996, lên khoảng 356 hecta.', Href='https://vnexpress.net/trong-san-ho-o-bien-cu-lao-cham-4563831.html', Title='Trồng san hô ở biển Cù Lao Chàm', Topic='Thời sự')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "320fd9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  2|  4|\n",
      "|  1|  2|\n",
      "|  5|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [\n",
    "    {\"a\": 1, \"b\": 2},\n",
    "    {\"a\": 5, \"b\": 4}\n",
    "]\n",
    "\n",
    "df2  = spark.createDataFrame(data2)\n",
    "\n",
    "df3 = df1.union(df2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a1c4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2938578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  2|  4|\n",
      "|  1|  2|\n",
      "|  5|  4|\n",
      "|  1|  2|\n",
      "|  5|  4|\n",
      "|  1|  2|\n",
      "|  5|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35735dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"save to HDFS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cee763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
